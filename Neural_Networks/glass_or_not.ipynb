{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "31fe01882e80be47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T15:53:10.110806Z",
     "start_time": "2025-01-14T15:53:08.722622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n"
   ],
   "id": "1e7bf95c2ea00519",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:40:06.674728Z",
     "start_time": "2025-01-12T13:40:00.438994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"../dataset_glasses_or_not\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "\n",
    "for data, _ in loader:\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ],
   "id": "dd22c97942ea369",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m std \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n\u001B[1;32m     21\u001B[0m nb_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_samples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    244\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[0;32m--> 245\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    247\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001B[0m, in \u001B[0;36mdefault_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:264\u001B[0m, in \u001B[0;36mpil_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    263\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/PIL/Image.py:984\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;15\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;16\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBGR;24\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    982\u001B[0m     deprecate(mode, \u001B[38;5;241m12\u001B[39m)\n\u001B[0;32m--> 984\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    986\u001B[0m has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n\u001B[1;32m    987\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    988\u001B[0m     \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:250\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mAttributeError\u001B[39;00m, \u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mImportError\u001B[39;00m):\n\u001B[1;32m    248\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_prepare\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m err_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m  \u001B[38;5;66;03m# initialize to unknown error\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap:\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# sort tiles in file order\u001B[39;00m\n",
      "File \u001B[0;32m~/Studia/V Semestr/ML/Project/project/.venv/lib/python3.12/site-packages/PIL/ImageFile.py:326\u001B[0m, in \u001B[0;36mImageFile.load_prepare\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;66;03m# create image memory if necessary\u001B[39;00m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_im \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 326\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;66;03m# create palette (optional)\u001B[39;00m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training model on training data",
   "id": "f468280f231fb568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:40:07.899020Z",
     "start_time": "2025-01-12T13:40:07.881707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_flattened_size(model, input_size):\n",
    "    dummy_input = torch.zeros(1, *input_size)\n",
    "    with torch.no_grad():\n",
    "        output = model.conv_layers(dummy_input)\n",
    "    return output.numel()\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.bn1,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            self.conv2,\n",
    "            self.bn2,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            self.conv3,\n",
    "            self.bn3,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            self.dropout1,\n",
    "        )\n",
    "\n",
    "        input_size = (3, 256, 256)\n",
    "        self.fc1_input_features = get_flattened_size(self, input_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc1_input_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) \n",
    "        optimizer.zero_grad() \n",
    "        output = model(data) \n",
    "        loss = F.nll_loss(output, target) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device) \n",
    "            output = model(data) \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def predict_image(model, device, image_path, transform, classes):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "\n",
    "    return classes[pred]\n",
    "\n",
    "\n",
    "def run(\n",
    "        batch_size=64,\n",
    "        test_batch_size=64,\n",
    "        epochs=5,\n",
    "        lr=0.0005, \n",
    "        gamma=0.7,\n",
    "        dry_run=False,\n",
    "        seed=1,\n",
    "        log_interval=10,\n",
    "        save_model=True,\n",
    "    ):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_kwargs = {'batch_size': batch_size, 'shuffle': True, 'pin_memory': False}\n",
    "    test_kwargs = {'batch_size': test_batch_size, 'shuffle': False, 'pin_memory': False}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5348, 0.4395, 0.3805), std=(0.2213, 0.1912, 0.1862))\n",
    "    ])\n",
    "\n",
    "    data_dir = \"../dataset_glasses_or_not\"\n",
    "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"../models/NN_glasses_or_not.pt\")\n"
   ],
   "id": "7715c292d2a4e106",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T15:17:01.430116Z",
     "start_time": "2025-01-12T13:40:08.585525Z"
    }
   },
   "cell_type": "code",
   "source": "run()\n",
   "id": "b52803bbb1e40d22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train Epoch: 1 [0/3444 (0%)]\tLoss: 0.709086\n",
      "Train Epoch: 1 [640/3444 (19%)]\tLoss: 1.133113\n",
      "Train Epoch: 1 [1280/3444 (37%)]\tLoss: 0.383145\n",
      "Train Epoch: 1 [1920/3444 (56%)]\tLoss: 0.063329\n",
      "Train Epoch: 1 [2560/3444 (74%)]\tLoss: 0.086083\n",
      "Train Epoch: 1 [3200/3444 (93%)]\tLoss: 0.037026\n",
      "\n",
      "Test set: Average loss: 0.0560, Accuracy: 1445/1477 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/3444 (0%)]\tLoss: 0.047133\n",
      "Train Epoch: 2 [640/3444 (19%)]\tLoss: 0.042900\n",
      "Train Epoch: 2 [1280/3444 (37%)]\tLoss: 0.022203\n",
      "Train Epoch: 2 [1920/3444 (56%)]\tLoss: 0.019703\n",
      "Train Epoch: 2 [2560/3444 (74%)]\tLoss: 0.007543\n",
      "Train Epoch: 2 [3200/3444 (93%)]\tLoss: 0.006639\n",
      "\n",
      "Test set: Average loss: 0.0110, Accuracy: 1470/1477 (100%)\n",
      "\n",
      "Train Epoch: 3 [0/3444 (0%)]\tLoss: 0.012862\n",
      "Train Epoch: 3 [640/3444 (19%)]\tLoss: 0.009867\n",
      "Train Epoch: 3 [1280/3444 (37%)]\tLoss: 0.009616\n",
      "Train Epoch: 3 [1920/3444 (56%)]\tLoss: 0.025382\n",
      "Train Epoch: 3 [2560/3444 (74%)]\tLoss: 0.035352\n",
      "Train Epoch: 3 [3200/3444 (93%)]\tLoss: 0.011313\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 1476/1477 (100%)\n",
      "\n",
      "Train Epoch: 4 [0/3444 (0%)]\tLoss: 0.017234\n",
      "Train Epoch: 4 [640/3444 (19%)]\tLoss: 0.003620\n",
      "Train Epoch: 4 [1280/3444 (37%)]\tLoss: 0.008474\n",
      "Train Epoch: 4 [1920/3444 (56%)]\tLoss: 0.001876\n",
      "Train Epoch: 4 [2560/3444 (74%)]\tLoss: 0.000851\n",
      "Train Epoch: 4 [3200/3444 (93%)]\tLoss: 0.004071\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 1477/1477 (100%)\n",
      "\n",
      "Train Epoch: 5 [0/3444 (0%)]\tLoss: 0.008286\n",
      "Train Epoch: 5 [640/3444 (19%)]\tLoss: 0.056058\n",
      "Train Epoch: 5 [1280/3444 (37%)]\tLoss: 0.009156\n",
      "Train Epoch: 5 [1920/3444 (56%)]\tLoss: 0.000398\n",
      "Train Epoch: 5 [2560/3444 (74%)]\tLoss: 0.000585\n",
      "Train Epoch: 5 [3200/3444 (93%)]\tLoss: 0.002101\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 1476/1477 (100%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f2aaf103de2bb5d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
